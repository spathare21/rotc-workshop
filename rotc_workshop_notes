K8s 

- minikube delete -p ''
- minikube start --memory 6000 --cpus=4 --driver=hyperkit
- minikube status
- minikube stop
- minikube delete

Start a cluster using the hyperkit driver:
minikube start --driver=hyperkit
To make hyperkit the default driver:
minikube config set driver hyperkit



Target /usr/local/bin/kubectl
already exists. You may want to remove it:
  rm '/usr/local/bin/kubectl'

To force the link and overwrite all conflicting files:
  brew link --overwrite kubernetes-cli

To list all files that would be deleted:
  brew link --overwrite --dry-run kubernetes-cli


HyperKit installation
* If Docker for Desktop is installed, you already have HyperKit
* Otherwise, if you have Brew Package Manager, run:
brew install hyperkit
* As a final alternative, you may Install HyperKit from GitHub
Usage
Start a cluster using the hyperkit driver:
minikube start --driver=hyperkit
To make hyperkit the default driver:
minikube config set driver hyperkit




Container Runtimes :

- Docker, rocket, cri.o 
- K8s, Mesos


Kubectl - it is kubernetes client used to communicate with k8s cluster


- Kubectl version --short  (get client and server version only)
- Kubectl get pods
- Kubectl get pods -o wide
- Kubectl get nodes 
- Kubectl get nodes -o wide

Use specific context 
    - Kubectl config use-context minikube(context name)

List all namespace (virtual seperation of pods)
- kubectl get pods --all-namespaces

All components from k8s control plane (etc/coredns/apiserver/schedulers) or master are running as pods in kube-system namespaces

Set namespace as default 

- Kubectl config set-context  --current --namespace=<namespace_name>

Apply created file 

- Kubectl apply -f file.yaml. or 
- Kubectl create -f file.yaml
      — create first time creation and when you have to update it kubectl update
     - apply - it will create if container is not exits and update if already exits 

Documentation : 
- Kubectl explain pods
- Kubectl explain pod.metadata
- Kubectl explain pod.metadata.name


When we create pods using kubectl create -f filename.yaml or kubectl apply -f filename.yaml
 - apiserver - store in etcd - scheduler - pick node - contact api server - talk to kubeclt and create pod in node 


- Kubectl create -f file.yaml
- Kubectl describe pod pod_name
- Kubectl logs pods_name (get logs of pod)

Port forwarding for pod 
- Kubectl port-forward pod_name 4444:4444
- Check on which port container is running example to find out 4444
- Kubectl describe pod pod_name 
- kubectl port-forward pod-demo 9000:80
- Access  - http://localhost:9000/


Minikube ssh - to go into worker node 


Image Pull policy 
- Alyways - default - always pull from hub
- Never - use from cache
- IfNotPresent - pull only if not present

Execute into pods 
- Kubectl exec -it pod_name — bash
- Kubectl exec -it  -c container_name pod_name bash (in case of multicontainer and want to exec into 1) 
- e.g kubectl exec exercise-metadata -it -- bash 
-  kubectl exec exercise-metadata -it -- sh


Kube Config 

- cat ~/.kube/config 


Deployment 
- Kubectl get deploy

Replicasets :
- What happens when pod is deleted 
- Replicates ensures that specified number of pod are running at any given time
- Scaling 
- Reliability
- It keep on watching pods and whenever some pods goes down it ups another pod
- Control manager take cares of bringing up new pods when 1 pod goes down it has replication set manger to manage replica
- Reconciliation control loop 
- - Current state  - observe, act - Desire State


- Kubectl explain replicatsets 
- Kubectl edit rs replicaset_name (if you want to modify anything in replica)
- Kubectl get rs replica_name -o yaml
- Kubectl scale rs replicaset_name —replica 4 (count , if we want to scale replicas)
- Kubectl get rs
- 


Health Check Probs for Container :

- Readiness probe
- livenessProbe -  pods get restarted if 3 connective failures of pods
- kubectl explain pods.spec.containers.livenessProb

If node does not have enough resources replica sets will be in pending state for pods like if we added replicas: 20 , scheduler will do this work



Services :


- How to access application within cluster and outside cluster ? 
- Pods are mortal and when they die they are not restructure with same ip
- An abstract way to expose an application running on a set of pods by reliable network service 
- Exposes set of pods over network with reliable IP Port and  DNS
- Associated with pods using matching labels
- Services also used for inter pods (microservices) communication
- Services Type 
-   clusterIP : exposes services on cluster-internal IP only reachable within cluster
-   NodePort : exposes the service on each Nodes IP at static port  allows to reach from outside cluster by requesting nodeIP: nodePort
-   LoadBalancer : Exposes the service externally using a cloud provers load balancer 

  - kubectl get svc
  - kubectl describe svc service_name
  - 

Deployments : 

A Deployment is a higher-level concept that manages ReplicaSets and provides declarative upgrades and rollbacks to pods.

-  What is missing in ReplicaSets 
-- Updates with zero downtime (rolling update)
-- Rollbacks
- Rolling updates/upgrade it will have ReplicaSets R1 and R2 will terminate 1 pod in R1 and spawn 1 pod in R2 like wise it will do updates/upgrade
- Rollback
-- R1 and R2 Delete 1 pod in R2 and spawn in R1 likewise it will rollback all pods/changes
- Problem with ReplicaSet is if we deploy new spec changes it does not get automatically deploy new one we need to delete pods and deploy rs again
- To solve above problem we have to use Deployments 

- kubectl get deploy 
- kubectl get rs (deployment creates replicaSets and let deploment manages replicasets)
- kubectl rollout --help
- kubectl rollout history deployment dobby 
- kubectl rollout undo deployment dobby (will rollback last deployment version)
- kubectl rollout deployment dobby --revision=6


- Deployment with annotation
- Deployment with strategy
- kubectl sclae deployment do by --replicas=6
- deployment recreate - all pods will terminate and all will be recreating and it will have downtime for application



Volume :

- emptyDir 
  - Created at pod creation time and when mounteed accessible by all container in pod 
  - Help with sharing data across containers withing pod

- hostPath
  - mounted inside container pod 
  - data retained on node even after pod goes down 
  - data is not available if pod schedule into another node 
  - can't save data from node outage
- awsElasticBlockStore
 - on the cloud save outage node 



Config Map and Secrets :




Daemonsets :

- Logs collections 
- monitoring
- its kind of deployment its ensures every single node in cluster runs the same pod resources
- When node is added , a new pod will be started automatically 
- When node is removed , the pod will not be resheduled on another node 

- kubectl get daemonsets 
- in single node cluster like minikube it will create only 1 pod 
- in mutlinode cluster it will create same number pods as number of nodes on each node



Jobs :

- Creates 1 or mote pod and make sure it terminates 
- job complete when all pods are terminate
- Job is nothing but controller over pod
- backPOffLimit - retries number of times on failuer - and after number off limit failure job will mark as failed  e.g backOffLimit=2 , 1st run and 2 retries total 3 failures 

- kubectl get jobs 
- kubectl get pods 


StatefulSets :

- use for stateful application like anything persist data
- Challenges without statefulsets 
 - loes data when container terminated
 - persistent volume are either shared or fresh every time
 - identity keeps changin as container come and go
 - ordering of replica et bootup is not fixed 
- e.g all above limitations are challenges to mongodb, postgres, elasticSearch, Kafka 
- 

Headless Service :
- client in the cluster can address the pod by its hostnmae
- static ip


 - Stable and unique network identifier
 - stable persistance storage
 - order gracefull deployment and scaling 
 - ordered gracefull deletion and termination
 - order automated rolling updates




















